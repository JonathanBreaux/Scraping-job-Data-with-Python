{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jonathanbreaux/scraping-job-data-with-python?scriptVersionId=196108765\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Small project: scrape job listing data with python.","metadata":{}},{"cell_type":"markdown","source":"## Summary\n    \n   I want to be able to run a program that collects job titles, posting dates, and company names into a dataframe for manipulation and a csv for ease of access. This project meets that goal, and establishes a simple framework upon which modules made to meet more advanced demands may be integrated later on.\n    \n## Approach\n\n   This is my first public project! So I took a student's approach. I looked up what tools would enable me to implement the functionalities I wanted to utilize. From there, I looked up documentation to understand how to operate those tools. This code is heavily documented as a result of my learning process.\n   \n## Solution\n\nMost of the top job-posting websites contain requests not to crawl their data, either in their \\[\"sitename.com\"]/robots.txt file or in their Terms and Conditions of use. \n\nSo we're using The Muse for this project, who offers a [fantastic API](https://www.themuse.com/developers/api/v2) that enables us to legally and ethically access and gather information directly from their website. Since The Muse returns structured JSON data for us, we can bypass the common step of utilizing BeautifulSoup to parse the HTML that most sites return.\n\nRegistering our app and using The Muse's provided API key allows us to make up to 3600 requests per hour to their API. Omitting this step limits us to the base maximum of 500 requests per hour. This project utilizes an API key, which will be stored and accessed through Kaggle's \"secrets\" add-on.","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport requests # allows for making HTTP requests\nimport pandas as pd # for DataFrames\nimport json # for heirarchichal format parsing\nimport time # for implementing time-based tools","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T18:27:43.501682Z","iopub.execute_input":"2024-09-10T18:27:43.502129Z","iopub.status.idle":"2024-09-10T18:27:44.064572Z","shell.execute_reply.started":"2024-09-10T18:27:43.502083Z","shell.execute_reply":"2024-09-10T18:27:44.063458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient  # for masking private data in workbook\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret('key')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T18:36:50.956613Z","iopub.execute_input":"2024-09-10T18:36:50.957798Z","iopub.status.idle":"2024-09-10T18:36:51.244368Z","shell.execute_reply.started":"2024-09-10T18:36:50.957745Z","shell.execute_reply":"2024-09-10T18:36:51.243015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define URL to The Muse API (with filters pre-selected)\nurl = 'https://www.themuse.com/api/public/jobs?category=Data%20and%20Analytics&category=Data%20Science&category=Writer&category=Writing%20and%20Editing&level=Entry%20Level&level=Mid%20Level&location=Flexible%20%2F%20Remote&page=0'\n# Pass api_key using key provided by registering your APP on The Muse\nparams = {'api_key': secret_value}\n# Send request to the API\nresponse = requests.get(url, params=params)\n# Parse a successful response\nif response.status_code == 200:\n    data = response.json()\n# Define list to store gathered data\n    job_data = []\n# Optional: check for successful API key pass with: print(response.headers)\n# 'x-ratelimit-limit' header will be 12000 with API key, and 500 without.\n# Extract critical information from parsed results\n    for job in data['results']:\n        title = job.get('name', 'N/A')\n        company = job.get('company', {}).get('name', 'N/A')\n        location = job.get('locations', [{'name': 'N/A'}])[0].get('name', 'N/A')\n        level = job.get('levels', [{'name': 'N/A'}])[0].get('name', 'N/A')\n        category = job.get('categories', [{'name': 'N/A'}])[0].get('name', 'N/A')\n        publication_date = job.get('publication_date', 'N/A')\n# Add collected information as a dictionary to job_data list\n        job_data.append({\n            'Title': title,\n            'Company': company,\n            'Level': level,\n            'Category': category,\n            'Publication Date': publication_date\n        })\n# Create Pandas DataFrame with list of dictionaries from job_data.append\n    df = pd.DataFrame(job_data)\n# Print first few lines of result to check\n    print(df.head())\n# Save to CSV file for further exploration\n    df.to_csv('job_postings.csv', index=False)    \n# Identify an unsuccessful response\nelse:\n    print(f\"Failed to retrieve: {response.status_code}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T18:42:10.161602Z","iopub.execute_input":"2024-09-10T18:42:10.162067Z","iopub.status.idle":"2024-09-10T18:42:10.538647Z","shell.execute_reply.started":"2024-09-10T18:42:10.162021Z","shell.execute_reply":"2024-09-10T18:42:10.537486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, looks like the code functions as intended. But collecting twenty postings at a time and then having to manually manipulate the URL isn't going to produce results that make this program worth the effort. \n\nFirst, I'm going to remove the .csv I created. I don't think this is particularly important but it seems neater.","metadata":{}},{"cell_type":"code","source":"import os\nos.remove('job_postings.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-10T18:50:29.251211Z","iopub.execute_input":"2024-09-10T18:50:29.251684Z","iopub.status.idle":"2024-09-10T18:50:29.257578Z","shell.execute_reply.started":"2024-09-10T18:50:29.251638Z","shell.execute_reply":"2024-09-10T18:50:29.256281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Refreshing my output directory reveals a blank slate. Cool. Now let's rework this code to implement pagination so that we can collect all of the results we want in one go.","metadata":{}},{"cell_type":"code","source":"# Replace the page number variable in the URL with an iterable placeholder\nbase_url = 'https://www.themuse.com/api/public/jobs?category=Data%20and%20Analytics&category=Data%20Science&category=Writer&category=Writing%20and%20Editing&level=Entry%20Level&level=Mid%20Level&location=Flexible%20%2F%20Remote&page={}'\nparams = {'api_key': secret_value}\n# Define list to store gathered data\njob_data = []\n# Start pagination from page 0\npage = 0\n# Define number of pages to include in report\nmax_pages = 20 \n# Define while loop \nwhile True:\n# Use str.format method to generate URL with iterated value in placeholder\n    url = base_url.format(page)\n# Send request to the API\n    response = requests.get(url, params=params)   \n# Parse a successful response\n    if response.status_code == 200:\n        data = response.json()        \n# Check results, break loop if no listings exist there\n        if not data['results']:\n            print(f\"No listings beyond page {page}.\")\n            break\n# Extract relevant information from each job posting using .get() method\n        for job in data['results']:\n            title = job.get('name', 'N/A') # 'N/A' allows the script to continue if !name\n            company = job.get('company', {}).get('name', 'N/A') # Fetch company from job dictionary, fetch name from retrieved dictionary\n            location = job.get('locations', [{'name': 'N/A'}])[0].get('name', 'N/A') # Fetch list of location dictionaries, fetch name from first result\n            level = job.get('levels', [{'name': 'N/A'}])[0].get('name', 'N/A')\n            category = job.get('categories', [{'name': 'N/A'}])[0].get('name', 'N/A')\n            publication_date = job.get('publication_date', 'N/A')\n# Add collected information as a dictionary to job_data list\n            job_data.append({\n                'Title': title,\n                'Company': company,\n                'Location': location,\n                'Level': level,\n                'Category': category,\n                'Publication Date': publication_date\n            })\n# Track progress of execution\n        print(f\"Page {page} added.\")\n# Iterate\n        page += 1\n# Add a delay timer to reduce demand on server\n        time.sleep(1)\n# (Optional) remove max_pages to scrape all available pages\n        if page > max_pages:\n            print(f\"Reached the page limit of {max_pages}.\")\n            break\n# Provide instructions for an unsuccessful response \n    else:\n        print(f\"Failed to retrieve data on page {page}: {response.status_code}\")\n        break\n# Create Pandas DataFrame with list of dictionaries from job_data.append\n    df = pd.DataFrame(job_data)\n# Print first few lines of result to check output\n# (Optional) indent this line to print header of each page during iteration\nprint(df.head())\n# Save the DataFrame to a CSV file\ndf.to_csv('job_postings.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T18:52:59.084323Z","iopub.execute_input":"2024-09-10T18:52:59.0849Z","iopub.status.idle":"2024-09-10T18:53:30.022103Z","shell.execute_reply.started":"2024-09-10T18:52:59.084845Z","shell.execute_reply":"2024-09-10T18:53:30.020866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":":)","metadata":{}}]}